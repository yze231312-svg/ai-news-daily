#!/usr/bin/env python3
"""
AI News Daily - æ–°é—»æŠ“å–ä¸æ€»ç»“è„šæœ¬
è‡ªåŠ¨ä»å¤šä¸ªæ¥æºæŠ“å– AI èµ„è®¯ï¼Œç”¨å¤§æ¨¡å‹æ€»ç»“æˆä¸­æ–‡ç®€æŠ¥
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

# å°è¯•å¯¼å…¥éœ€è¦çš„åº“
try:
    import requests
except ImportError:
    print("âŒ è¯·å®‰è£… requests: pip install requests")
    sys.exit(1)

# Tavily API Key (æ”¯æŒä¸­æ–‡æœç´¢)
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY", "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")
TAVILY_SEARCH_URL = "https://api.tavily.com/search"

# AI æ–°é—»æœç´¢å…³é”®è¯
SEARCH_QUERIES = [
    "artificial intelligence LLM model research breakthrough February 2026",
    "DeepSeek Claude GPT OpenAI news 2026",
    "AI coding assistant Cursor Copilot GitHub",
    "multimodal model vision language 2026",
    "AI agents autonomous workflow 2026",
    "machine learning research paper arxiv 2026",
    "Google Microsoft Anthropic AI product 2026",
    "local LLM quantization Ollama 2026"
]

# æ–°é—»æ¥æºæ˜ å°„
SOURCE_MAP = {
    "github.com": "GITHUB",
    "news.ycombinator.com": "HN",
    "arxiv.org": "ARXIV",
    "techmeme.com": "TECHMEME",
    "venturebeat.com": "VENTUREBEAT",
    "techcrunch.com": "TECHCRUNCH",
    "thedecoder.com": "DECODER",
    "reddit.com": "REDDIT",
    "anthropic.com": "ANTHROPIC",
    "openai.com": "OPENAI",
    "google.com": "GOOGLE",
    "microsoft.com": "MICROSOFT"
}

# æ¯æ—¥é‡‘å¥
DAILY_QUOTES = [
    ("AI won't replace humans, but humans using AI will replace those who don't.", "Anonymous"),
    ("The best way to predict the future is to create it.", "Peter Drucker"),
    ("In the future, there will be two types of people: those who use AI, and those who are used by AI.", "Sam Altman"),
    ("AI is the new electricity. It will transform every industry.", "Andrew Ng"),
    ("Don't worry about AI taking your job. Worry about someone using AI to take your job.", "Anonymous"),
    ("The intersection of AI and human creativity is where magic happens.", "Fei-Fei Li"),
    ("The most powerful tool we have is imagination.", "Geoffrey Hinton")
]


def extract_source(url: str) -> str:
    """ä» URL æå–æ¥æº"""
    try:
        from urllib.parse import urlparse
        hostname = urlparse(url).netloc.lower()
        for domain, source in SOURCE_MAP.items():
            if domain in hostname:
                return source
        # é»˜è®¤å¤„ç†
        return hostname.replace("www.", "").split(".")[0].upper()[:10]
    except:
        return "AI NEWS"


def format_date() -> str:
    """æ ¼å¼åŒ–æ—¥æœŸ"""
    return datetime.now().strftime("%Y/%m/%d")


def get_quote() -> dict:
    """è·å–æ¯æ—¥é‡‘å¥"""
    import random
    today = datetime.now().day
    quote = DAILY_QUOTES[today % len(DAILY_QUOTES)]
    return {
        "text": quote[0],
        "author": quote[1]
    }


def search_news(query: str, max_results: int = 5) -> list:
    """ä½¿ç”¨ Tavily æœç´¢æ–°é—»"""
    headers = {"Content-Type": "application/json"}
    
    payload = {
        "api_key": TAVILY_API_KEY,
        "query": query,
        "max_results": max_results,
        "include_domains": [
            "github.com",
            "news.ycombinator.com", 
            "arxiv.org",
            "techmeme.com",
            "venturebeat.com",
            "techcrunch.com",
            "anthropic.com",
            "openai.com",
            "google.com",
            "microsoft.com"
        ],
        "exclude_domains": ["facebook.com", "twitter.com", "x.com"]
    }
    
    try:
        response = requests.post(TAVILY_SEARCH_URL, json=payload, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        return data.get("results", [])
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥ ({query[:30]}...): {e}")
        return []


def summarize_with_ai(articles: list) -> list:
    """
    ç”¨å¤§æ¨¡å‹æ€»ç»“æ–‡ç« ï¼ˆç®€åŒ–ç‰ˆï¼šæ¸…ç†æè¿°ï¼‰
    å®Œæ•´ç‰ˆå¯ä»¥è°ƒç”¨ OpenAI/Gemini API è¿›è¡Œæ™ºèƒ½æ€»ç»“
    """
    summarized = []
    
    for article in articles[:15]:  # æœ€å¤š 15 æ¡
        title = article.get("title", "").strip()
        url = article.get("url", "")
        content = article.get("content", "") or article.get("snippet", "")
        
        if not title:
            continue
        
        # æ¸…ç†æè¿°
        description = content[:300].strip()
        if description:
            # å»é™¤å¤šä½™ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
            import re
            description = re.sub(r'\s+', ' ', description)
            description = description.replace("#", "").replace("*", "")
        
        summarized.append({
            "title": title,
            "url": url,
            "description": description,
            "source": extract_source(url),
            "date": format_date()
        })
    
    return summarized


def save_to_json(articles: list, output_file: str = "data.json"):
    """ä¿å­˜åˆ° JSON æ–‡ä»¶"""
    # æ–°é—»æ¥æºåˆ—è¡¨
    sources = [
        "ğŸ“° Hacker News (Y Combinator)",
        "ğŸ’» GitHub Trending", 
        "ğŸ“„ ArXiv (cs.AI)",
        "ğŸ“Š Techmeme",
        "ğŸ“ˆ VentureBeat / TechCrunch",
        "ğŸ¯ The Decoder",
        "ğŸ’¬ Reddit (r/ML, r/LocalLLaMA)"
    ]
    
    data = {
        "articles": articles,
        "lastUpdate": datetime.now().strftime("%Y/%m/%d %H:%M"),
        "sources": sources
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜ {len(articles)} æ¡æ–°é—»åˆ° {output_file}")


def update_html_quote():
    """æ›´æ–° HTML é¡µé¢çš„é‡‘å¥"""
    quote = get_quote()
    quote_js = f'''
    <script>
        document.addEventListener('DOMContentLoaded', () => {{
            const quotes = [
                {{ text: "{quote['text']}", author: "{quote['author']}" }}
            ];
            const today = new Date().getDate();
            const q = quotes[today % quotes.length];
            document.getElementById('quote-text').textContent = '"' + q.text + '"';
            document.getElementById('quote-author').textContent = 'â€” ' + q.author;
        }});
    </script>
'''
    return quote_js


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– AI News Daily - å¼€å§‹æŠ“å–æ–°é—»...")
    print("=" * 50)
    
    # æœç´¢æ‰€æœ‰æŸ¥è¯¢
    all_articles = []
    
    for i, query in enumerate(SEARCH_QUERIES, 1):
        print(f"ğŸ” [{i}/{len(SEARCH_QUERIES)}] æœç´¢: {query[:40]}...")
        results = search_news(query, max_results=3)
        all_articles.extend(results)
        print(f"   â†’ è·å– {len(results)} æ¡ç»“æœ")
    
    print("=" * 50)
    print(f"ğŸ“Š å…±è·å– {len(all_articles)} æ¡åŸå§‹æ–°é—»")
    
    # å»é‡
    seen = set()
    unique_articles = []
    for article in all_articles:
        key = article.get("url", article.get("title", ""))
        if key and key not in seen:
            seen.add(key)
            unique_articles.append(article)
    
    print(f"ğŸ”— å»é‡åå‰© {len(unique_articles)} æ¡")
    
    # æ€»ç»“
    print("ğŸ“ æ­£åœ¨æ€»ç»“...")
    summarized = summarize_with_ai(unique_articles)
    print(f"âœ… æ€»ç»“å®Œæˆï¼Œå…± {len(summarized)} æ¡")
    
    # ä¿å­˜
    output_file = os.environ.get("OUTPUT_FILE", "data.json")
    save_to_json(summarized, output_file)
    
    # æ‰“å°ç»Ÿè®¡
    sources_count = {}
    for article in summarized:
        source = article["source"]
        sources_count[source] = sources_count.get(source, 0) + 1
    
    print("\nğŸ“Š æ¥æºç»Ÿè®¡:")
    for source, count in sorted(sources_count.items(), key=lambda x: -x[1]):
        print(f"   {source}: {count}")
    
    print("\nğŸ‰ å®Œæˆï¼æ•°æ®å·²ä¿å­˜ï¼Œå¯æ¨é€åˆ° GitHub")


if __name__ == "__main__":
    main()
