#!/usr/bin/env python3
"""
AI News Daily - æ¯æ—¥äººå·¥æ™ºèƒ½èµ„è®¯
æ”¯æŒä¸­è‹±æ–‡æ ‡é¢˜ç¿»è¯‘
"""

import json
import feedparser
import requests
import re
from datetime import datetime
from collections import defaultdict

# åˆ†ç±»é…ç½®
CATEGORIES = {
    "model": {"name": "ğŸ—£ï¸ æ¨¡å‹å‘å¸ƒ", "keywords": ["gpt", "claude", "gemini", "llama", "model", "release", "openai", "anthropic", "mistral", "moe", "parameter", "billion", "open-source model", "api", "version", "capability", "benchmark"]},
    "tutorial": {"name": "ğŸ“– ä½¿ç”¨æ•™ç¨‹", "keywords": ["tutorial", "guide", "how to", "getting started", "setup", "configuration", "integration", "api call", "prompt engineering", "opencl", "installation", "documentation", "example", "tips", "best practice"]},
    "tool": {"name": "ğŸ› ï¸ å·¥å…·å¹³å°", "keywords": ["tool", "platform", "api", "sdk", "launch", "feature", "announcement", "framework", "library", "studio", "plugin", "app", "software"]},
    "research": {"name": "ğŸ“š ç ”ç©¶æˆæœ", "keywords": ["paper", "research", "arxiv", "study", "benchmark", "accuracy", "state-of-the-art", "sota", "performance", "training", "inference", "neural", "network", "learning", "dataset"]},
    "industry": {"name": "ğŸ“° è¡Œä¸šåŠ¨æ€", "keywords": ["ai", "microsoft", "google", "amazon", "meta", "nvidia", "startup", "funding", "valuation", "ipo", "acquisition", "partnership", "ceo", "executive", "company", "news"]}
}

# ç¿»è¯‘å™¨
def translate_to_cn(text):
    """ç¿»è¯‘æˆä¸­æ–‡"""
    if not text or len(text) < 5:
        return text
    
    try:
        # ä½¿ç”¨ MyMemory APIï¼ˆå…è´¹ï¼Œæ— éœ€ API Keyï¼‰
        url = f"https://api.mymemory.translated.net/get?q={requests.utils.quote(text)}&langpair=en|zh-CN"
        resp = requests.get(url, timeout=10)
        data = resp.json()
        if data.get('responseStatus') == 200:
            translated = data['responseData']['translatedText']
            if translated and translated != text:
                return translated
    except Exception as e:
        pass
    
    return text

SOURCES = [
    {"name": "GitHub Blog", "url": "https://github.blog/feed/", "type": "rss"},
    {"name": "HuggingFace", "url": "https://huggingface.co/api/models?sort=downloads&direction=-1&limit=30&filter=featured", "type": "huggingface"},
    {"name": "arXiv AI", "url": "http://export.arxiv.org/api/query?search_query=cat:cs.AI&sortBy=submittedDate&sortOrder=-1&limit=25", "type": "arxiv"},
    {"name": "arXiv ML", "url": "http://export.arxiv.org/api/query?search_query=cat:cs.LG&sortBy=submittedDate&sortOrder=-1&limit=20", "type": "arxiv"},
    {"name": "VentureBeat AI", "url": "https://venturebeat.com/category/ai/feed/", "type": "rss"},
    {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed", "type": "rss"},
    {"name": "MIT Tech Review", "url": "https://www.technologyreview.com/feed/", "type": "rss"},
    {"name": "The Verge", "url": "https://www.theverge.com/rss/index.xml", "type": "rss"},
    {"name": "Product Hunt", "url": "https://www.producthunt.com/category/artificial-intelligence/feed", "type": "rss"},
]

def fetch_huggingface(url, source_name):
    try:
        resp = requests.get(url, timeout=15)
        data = resp.json()
        articles = []
        for m in data[:25]:
            title = m.get('modelId', 'Unknown')
            articles.append({
                "title": translate_to_cn(title),
                "original_title": title,
                "url": f"https://huggingface.co/{m.get('modelId', '')}",
                "source": "HuggingFace",
                "date": datetime.now().isoformat(),
                "summary": f"Downloads: {m.get('downloads', 0):,} | Likes: {m.get('likes', 0):,}",
                "tags": ["Model", "AI"]
            })
        return articles
    except Exception as e:
        print(f"  âš ï¸ HuggingFace: {e}")
        return []

def fetch_arxiv(url, source_name):
    try:
        feed = feedparser.parse(url)
        articles = []
        for e in feed.entries[:20]:
            title = e.title
            articles.append({
                "title": translate_to_cn(title),
                "original_title": title,
                "url": e.link,
                "source": "arXiv",
                "date": e.published if hasattr(e, 'published') else datetime.now().isoformat(),
                "summary": translate_to_cn(e.summary[:200] + "...") if hasattr(e, 'summary') else "",
                "tags": ["Research", "AI"]
            })
        return articles
    except Exception as e:
        print(f"  âš ï¸ arXiv: {e}")
        return []

def fetch_rss(url, source_name):
    try:
        feed = feedparser.parse(url)
        articles = []
        for e in feed.entries[:15]:
            title = e.title
            articles.append({
                "title": translate_to_cn(title),
                "original_title": title,
                "url": e.link,
                "source": source_name,
                "date": e.published if hasattr(e, 'published') else datetime.now().isoformat(),
                "summary": translate_to_cn(e.summary[:200] + "...") if hasattr(e, 'summary') else "",
                "tags": []
            })
        return articles
    except Exception as e:
        print(f"  âš ï¸ {source_name}: {e}")
        return []

def classify(article, categories):
    # arXiv æ¥æºçš„è®ºæ–‡ä¼˜å…ˆå½’ç±»ä¸ºç ”ç©¶æˆæœ
    if article.get('source') == 'arXiv':
        return 'research'
    
    text = (article.get('title', '') + ' ' + article.get('original_title', '') + ' ' + article.get('summary', '')).lower()
    scores = {}
    for cat, config in categories.items():
        score = sum(1 for k in config.get('keywords', []) if k.lower() in text)
        scores[cat] = score
    if scores:
        best = max(scores, key=scores.get)
        return best if scores[best] > 0 else 'industry'
    return 'industry'

def fetch_all():
    all_articles = defaultdict(list)
    seen = set()
    
    print("\nğŸš€ å¼€å§‹è·å–èµ„è®¯...\n")
    
    for src in SOURCES:
        print(f"ğŸ“¥ {src['name']}...", end=" ", flush=True)
        if src['type'] == 'arxiv':
            arts = fetch_arxiv(src['url'], src['name'])
        elif src['type'] == 'huggingface':
            arts = fetch_huggingface(src['url'], src['name'])
        else:
            arts = fetch_rss(src['url'], src['name'])
        print(f"{len(arts)} æ¡")
        
        for a in arts:
            if a['url'] in seen:
                continue
            seen.add(a['url'])
            a['category'] = classify(a, CATEGORIES)
            a['category_name'] = CATEGORIES[a['category']]['name']
            all_articles[a['category']].append(a)
    
    result = {
        "lastUpdate": datetime.now().strftime("%Y/%m/%d %H:%M"),
        "categories": [],
        "articles": []
    }
    
    for cat, config in CATEGORIES.items():
        result['categories'].append({
            "key": cat,
            "name": config['name'],
            "count": len(all_articles[cat])
        })
        result['articles'].extend(all_articles[cat])
    
    result['articles'].sort(key=lambda x: x['date'], reverse=True)
    
    return result

def main():
    data = fetch_all()
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… æ›´æ–°å®Œæˆï¼å…± {len(data['articles'])} æ¡")
    for c in data['categories']:
        print(f"   {c['name']}: {c['count']} æ¡")

if __name__ == "__main__":
    main()
