#!/usr/bin/env python3
"""
AI News Daily - å¤šæºèµ„è®¯èšåˆ
åˆ†ç±»ï¼šæ¨¡å‹å‘å¸ƒã€å·¥å…·å¹³å°ã€ç ”ç©¶æˆæœã€å¼€æºé¡¹ç›®ã€è¡Œä¸šåŠ¨æ€
"""

import json
import feedparser
import requests
import re
from datetime import datetime
from collections import defaultdict

# åˆ†ç±»é…ç½®
CATEGORIES = {
    "model": {
        "name": "ğŸ—£ï¸ æ¨¡å‹å‘å¸ƒ",
        "name_en": "Model Releases",
        "sources": [
            {"name": "HuggingFace", "url": "https://huggingface.co/api/models?sort=downloads&direction=-1&limit=20&filter=featured", "type": "api"},
            {"name": "OpenAI", "url": "https://feeds.feedburner.com/OpenAi", "type": "rss"},
            {"name": "Anthropic", "url": "https://www.anthropic.com/rss.xml", "type": "rss"},
            {"name": "Google AI", "url": "http://googleaiblog.blogspot.com/atom.xml", "type": "rss"},
        ],
        "keywords": ["gpt", "claude", "gemini", "llama", "model", "release", "openai", "anthropic", "google deepmind", "mistral"]
    },
    "tool": {
        "name": "ğŸ› ï¸ å·¥å…·å¹³å°",
        "name_en": "Tools & Platforms",
        "sources": [
            {"name": "Product Hunt AI", "url": "https://www.producthunt.com/category/artificial-intelligence/feed", "type": "rss"},
            {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed", "type": "rss"},
            {"name": "Vercel", "url": "https://vercel.com/blog/rss.xml", "type": "rss"},
        ],
        "keywords": ["tool", "platform", "api", "sdk", "launch", "feature", "announcement", "release", "new"]
    },
    "research": {
        "name": "ğŸ“š ç ”ç©¶æˆæœ",
        "name_en": "Research Papers",
        "sources": [
            {"name": "ArXiv AI", "url": "http://export.arxiv.org/api/query?search_query=cat:cs.AI&sortBy=submittedDate&sortOrder=-1&max_results=30", "type": "arxiv"},
            {"name": "ArXiv ML", "url": "http://export.arxiv.org/api/query?search_query=cat:cs.LG&sortBy=submittedDate&sortOrder=-1&max_results=20", "type": "arxiv"},
            {"name": "Google Research", "url": "https://research.google/blog/rss.xml", "type": "rss"},
            {"name": "Meta AI", "url": "https://ai.meta.com/blog/rss.xml", "type": "rss"},
        ],
        "keywords": ["paper", "research", "arxiv", "study", "benchmark", "performance", "accuracy", "state-of-the-art"]
    },
    "opensource": {
        "name": "ğŸ“¦ å¼€æºé¡¹ç›®",
        "name_en": "Open Source",
        "sources": [
            {"name": "GitHub Trending", "url": "https://github.com/trending?since=daily", "type": "github"},
            {"name": "GitHub Python", "url": "https://github.com/trending/python?since=daily", "type": "github"},
            {"name": "GitHub JavaScript", "url": "https://github.com/trending/javascript?since=daily", "type": "github"},
        ],
        "keywords": ["github", "stars", "repository", "repo", "github.com"]
    },
    "industry": {
        "name": "ğŸ“° è¡Œä¸šåŠ¨æ€",
        "name_en": "Industry News",
        "sources": [
            {"name": "VentureBeat AI", "url": "https://venturebeat.com/category/ai/feed/", "type": "rss"},
            {"name": "MIT Tech Review", "url": "https://www.technologyreview.com/feed/", "type": "rss"},
            {"name": "The Verge AI", "url": "https://www.theverge.com/rss/index.xml", "type": "rss"},
            {"name": "Wired AI", "url": "https://www.wired.com/feed/tag/ai/latest/rss", "type": "rss"},
        ],
        "keywords": ["ai", "artificial intelligence", "chatgpt", "openai", "microsoft", "google", "amazon", "meta", "nvidia", "startup"]
    }
}

def fetch_rss(url, source_name):
    """è·å– RSS è®¢é˜…æº"""
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:15]:
            # æå–æ ‡ç­¾
            tags = [tag.term for tag in entry.tags] if hasattr(entry, 'tags') else []
            if not tags and hasattr(entry, 'categories'):
                tags = entry.categories
            
            articles.append({
                "title": entry.title,
                "url": entry.link,
                "source": source_name,
                "date": entry.published if hasattr(entry, 'published') else str(datetime.now()),
                "summary": entry.summary[:200] + "..." if hasattr(entry, 'summary') else "",
                "tags": tags[:5]
            })
        return articles
    except Exception as e:
        print(f"Error fetching {source_name}: {e}")
        return []

def fetch_github_trending(url, source_name):
    """è·å– GitHub Trending"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        articles = []
        
        # è§£æ GitHub Trending é¡µé¢
        pattern = r'<article class="Box-row">.*?href="([^"]+)"[^>]*>\s*<h2[^>]*>([^<]+)</h2>.*?<p class="col-\d+[^"]*"[^>]*>([^<]+)</p>'
        matches = re.findall(pattern, response.text, re.DOTALL)
        
        for match in matches[:10]:
            repo_url, title, desc = match
            articles.append({
                "title": title.strip(),
                "url": f"https://github.com{repo_url}",
                "source": "GitHub Trending",
                "date": str(datetime.now()),
                "summary": desc.strip()[:200] if desc else "",
                "tags": ["GitHub", "Trending"]
            })
        return articles
    except Exception as e:
        print(f"Error fetching GitHub: {e}")
        return []

def fetch_arxiv(url, source_name):
    """è·å– ArXiv è®ºæ–‡"""
    try:
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:15]:
            # æå– ArXiv ID å’Œæ ‡ç­¾
            arxiv_id = entry.id.split('/abs/')[-1] if '/abs/' in entry.id else entry.id
            
            # è·å–è®ºæ–‡ PDF é“¾æ¥
            pdf_url = entry.link.replace('/abs/', '/pdf/') if '/abs/' in entry.link else entry.link
            
            # æå–æ ‡ç­¾ï¼ˆåˆ†ç±»ï¼‰
            tags = []
            if hasattr(entry, 'tags'):
                for tag in entry.tags:
                    if tag.term:
                        tags.append(tag.term)
            
            articles.append({
                "title": entry.title,
                "url": pdf_url,
                "source": "arXiv",
                "date": entry.published if hasattr(entry, 'published') else str(datetime.now()),
                "summary": entry.summary[:200] + "..." if hasattr(entry, 'summary') else "",
                "tags": tags[:5] if tags else ["AI", "Research"]
            })
        return articles
    except Exception as e:
        print(f"Error fetching ArXiv: {e}")
        return []

def fetch_huggingface(url, source_name):
    """è·å– HuggingFace çƒ­é—¨æ¨¡å‹"""
    try:
        response = requests.get(url, timeout=10)
        data = response.json()
        articles = []
        
        for model in data[:20]:
            # ç”Ÿæˆæ ‡ç­¾
            tags = model.get('tags', [])
            if 'transformers' in tags:
                tags = ['Transformers'] + [t for t in tags if t != 'transformers']
            elif 'diffusers' in tags:
                tags = ['Diffusion'] + [t for t in tags if t != 'diffusers']
            
            articles.append({
                "title": model.get('modelId', 'Unknown Model'),
                "url": f"https://huggingface.co/{model.get('modelId', '')}",
                "source": "HuggingFace",
                "date": str(datetime.now()),
                "summary": f"Downloads: {model.get('downloads', 0):,} | Likes: {model.get('likes', 0):,}",
                "tags": tags[:5] if tags else ["Model", "AI"]
            })
        return articles
    except Exception as e:
        print(f"Error fetching HuggingFace: {e}")
        return []

def classify_article(article, categories):
    """æ ¹æ®å…³é”®è¯å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»"""
    title = article.get('title', '').lower()
    summary = article.get('summary', '').lower()
    text = title + " " + summary
    
    scores = {}
    for cat_key, config in categories.items():
        score = 0
        for keyword in config.get('keywords', []):
            if keyword.lower() in text:
                score += 1
        scores[cat_key] = score
    
    # è¿”å›åˆ†æ•°æœ€é«˜çš„åˆ†ç±»
    if scores:
        best_cat = max(scores, key=scores.get)
        return best_cat if scores[best_cat] > 0 else None
    return None

def fetch_all_news():
    """è·å–æ‰€æœ‰æ–°é—»å¹¶åˆ†ç±»"""
    all_articles = defaultdict(list)
    seen_urls = set()
    
    for cat_key, config in CATEGORIES.items():
        print(f"Fetching {config['name']}...")
        
        for source in config['sources']:
            if source['type'] == 'rss':
                articles = fetch_rss(source['url'], source['name'])
            elif source['type'] == 'github':
                articles = fetch_github_trending(source['url'], source['name'])
            elif source['type'] == 'arxiv':
                articles = fetch_arxiv(source['url'], source['name'])
            elif source['type'] == 'api':
                articles = fetch_huggingface(source['url'], source['name'])
            else:
                articles = []
            
            for article in articles:
                # å»é‡
                if article['url'] in seen_urls:
                    continue
                seen_urls.add(article['url'])
                
                # è‡ªåŠ¨åˆ†ç±»
                article['category'] = cat_key
                article['category_name'] = config['name']
                
                all_articles[cat_key].append(article)
    
    # æ„å»ºæœ€ç»ˆæ•°æ®
    result = {
        "lastUpdate": datetime.now().strftime("%Y/%m/%d %H:%M"),
        "categories": [],
        "articles": []
    }
    
    for cat_key, config in CATEGORIES.items():
        category_data = {
            "key": cat_key,
            "name": config['name'],
            "name_en": config['name_en'],
            "count": len(all_articles[cat_key])
        }
        result['categories'].append(category_data)
        result['articles'].extend(all_articles[cat_key])
    
    # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°çš„åœ¨å‰ï¼‰
    result['articles'].sort(key=lambda x: x['date'], reverse=True)
    
    return result

def main():
    data = fetch_all_news()
    
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ›´æ–°å®Œæˆï¼")
    print(f"ğŸ“Š æ€»è®¡: {len(data['articles'])} æ¡æ–°é—»")
    for cat in data['categories']:
        print(f"   {cat['name']}: {cat['count']} æ¡")

if __name__ == "__main__":
    main()
